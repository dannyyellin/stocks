# The program uses the LangGraph framework to generate code for a microservice S and test it as follows:
# 1. Invoke graphInit to initialize graph state
# 2. Invoke generate_microservice to synthesize the microservice S code per requirements and code generation guidelines.
#    Store the generated microservice in a file as well as in graph state.
# 3. Invoke run_docker_and_tests:
#    (i) copies the generated code for S to the appropriate directory, It also generates the requirements.txt file for
#    S for the Dockefile to use.
#    (ii) removes existing docker image for S if one already exists.  removes any existing containers before starting up
#    the containers for running S.   runs docker compose to execuate the code S and any other necessary containers.
#    (iii) tests the system using unit tests.
#    (iv) finds and records any run-time and test errors when running the unit test logs.   It does this as follows:
#       a.  collect the output from the unit tests (on stdout).   extract the output starting from the keyword
#           "Traceback". then "compress" these error messages (as it can be voluminous and contains information that is
#           hard to use for debugging).  The compression finds lines containing "S.py", where S is the name of the
#           service being generated (we assume the code being tested is in a file of that name).   It will take the line
#           containing "S.py" and the next 4 lines, as usually that suffices to describe the error.
#       b.  collect the docker log files from the docker container for S.    follow the same process as in (a) to
#           collect and compress error messages.
#           **** NEED TO CHECK that there is not overlap between (a) and (b)  ********
#       c.  append the error messages from (b) to those from (a) to form the runtime error messages RT.  if RT is empty,
#           there are no RT error messages.  If it is not empty, then set the error messages E to be RT.
#       d.  if there are not any RT errors, then assuming there are test errors TE (report from the unit tests on failed
#           tests), set E to TE together with the direct output from the unit tests (the output it prints to stdout).
#    (v) shuts down the containers and removes them using docker compose down command.
# 4. Execute agent Chk4rErr.   If there are errors (either runtime errors or test errors) and the program has not reached
#    MAX_RETRIES, then go to 5. Otherwise, if no errors or have errors and MAX_RETRIES reached, go to (7).
# 5  Execute agent ReflectOnErr.   Reflect on the error messages.  Store the result of the reflection.
# 6. Using revised prompt, execute agent ReGenQueryPgm, generating a new program taking the errors and reflection into
#    consideration. Goto (3).
# 7. Execute agent FinalizeOut.   Finalize outputs and go to END.
#
#  Throughout these steps, metadata is collected and printed to a metadata file (mostly on model related statistics,
#  like number of tokens used,...).
#
# Output of program is 5 files (per code generation), plus one metadata file.   These are stored in the subdirectory
# <timestamp>-<model>.# Example: "16-08-2024?-4852-gpt-35-16k"
# a. <service-name>-s<n1>-v<n2>.py is the generated code.   There can be more than one file, as n1 is the sample number
#    of the generated code (we are only using s=1 for now), and n2 ranges from 0 to MAX_RETRIES;  I.e., for n2 > 0,
#    the file contains the regenerated code (i.e., version n2-1 had errors, and we have not yet reached MAX_RETRIES).
# b. <service-name>-s<n1>-v<n2>-runtime-errors.txt is the python runtime errors (including docker log file) containing
#    Python Traceback errors for the generated program <service-name>-s<n1>-v<n2>.py.  This file is only generated if
#    the Python execution generated Traceback errors.
# c. <service-name>-s<n1>-v<n2>-test-errors.txt is the list of failed unit tests for <service-name>-s<n1>-v<n2>.py with
#    a message explaining the failure.  This file is generated by the unit test script.   This file is only generated
#    if there are any unit tests that fail.
# d. <service-name>-s<n1>-v<n2>-test-log.txt is the log file providing details on which tests passed or failed and
#    includes generated statistics.  This file is generated by the unit test script.
# e. <service-name>-s<n1>-v<n2>-reflect.txt is the generated reflection the describes the cause of the errors in
#    <service-name>-s<n1>-v<n2>-runtime-errors.txt
# f. meta_data is a text file that logs information about the parameters of the various LLM invocations.

# to use this program you need to set:
# 1. the base_dir variable (bottom of file), 2. the SERVICES variable (top of file) listing all the services to generate
# code for, (3) the test_prog state vaariable giving the location of the test script (in the langgraph node graphInit),
# (4) any changes you want to the meta variables controlling which model, temperature etc. to use, as well the number of
# retries.

# under the base dir are the subdirectories:
# - <service_name>: containing the ground truth code and the requirements.txt file for that service, and the Dockerfile
#    for that service.  The LLM generated service and its requirements.txt file will also be placed there.
# -  LLMgeneratedAndTest: containing this code.
# - tests: containing the testing code
# - prompts:  containing LLM prompts used by this program

# ** GENERALIZE ** comments in the code for places that need to be generalized, fixed
import shutil
import time
from helpers import get_section, has_multiple_lines, extract_errors, generate_requirements_file
from docker_funcs import list_containers, is_container_running, remove_all_containers, list_images, remove_image
from openai import AzureOpenAI
from datetime import datetime
from typing import TypedDict
from langgraph.graph import StateGraph, END
import os
import subprocess

# --------- Initialize variables controlling model, temperature, tokens, number of retries, and microservice --------#
# Set up GPT4 model
AZURE_OPENAI_API_KEY = os.getenv('SUBSCRIPTION_OPENAI_API_KEY')
AZURE_OPENAI_ENDPOINT = os.getenv('SUBSCRIPTION_OPENAI_ENDPOINT')
OPENAI_API_VERSION = '2023-12-01-preview'
GPT35 = True
# GPT35 = False   # if not GPT35 then GPT4
# TEMP = 0
# TEMP = .3
TEMP = .5

# Choose model to use
# MODEL = 'gpt-35-16k'   # Model Name: gpt-35-turbo-16k.   Modified on: Apr 15, 2024 4:17 PM    Max Tokens: 16,384
# MAX_TOKENS = 10000
# the following was error message for gpt-35-16k when i set max-tokens too high - you cannot set it to 16k
#   openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16384 tokens.
# However, you requested 18183 tokens (2183 in the messages, 16000 in the completion). Please reduce the length of the
# messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
#   openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16384 tokens.
# However, you requested 16620 tokens (4620 in the messages, 12000 in the completion). Please reduce the length of the
# messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
# similarly for gpt4 - more than 6000 problematic.   Here is error message when I set it to 7500:
#   openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens.
# However, you requested 9649 tokens (2149 in the messages, 7500 in the completion). Please reduce the length of the
# messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
# MODEL = 'gpt4'       # Model Name: gpt-4.              Modified on: May 15, 2024 3:45 PM.   Max Tokens:  8,192 tokens.
# gpt-4-0613: gpt-4 model with a context window of 8,192 tokens. Training data up to September 2021.
#   openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens.
# However, you requested 8637 tokens (2637 in the messages, 6000 in the completion). Please reduce the length of the
# messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
# even MAX_TOKENS 4100 was too much for reflection
# can set it to 6000 for generating code and less for reflect invocation/regen invocation.   or shorten prompt
MAX_TOKENS_GPT35_1 = 7000
MAX_TOKENS_GPT35_2 = 6000
MAX_TOKENS_GPT4_1 = 5000
MAX_TOKENS_GPT4_2 = 4000
if GPT35:
    MODEL = 'gpt-35-16k'
    MAX_TOKENS_1 = MAX_TOKENS_GPT35_1
    MAX_TOKENS_2 = MAX_TOKENS_GPT35_2
else:   # using GPT4 with less tokens
    MODEL = 'gpt4'
    MAX_TOKENS_1 = MAX_TOKENS_GPT4_1
    MAX_TOKENS_2 = MAX_TOKENS_GPT4_2

NUM_SAMPLES = 1    # number of samples code LLM should generate in one invocation
MAX_RETRIES = 1    # max number of times to retry to generate error-free program
SERVICES = ['stocks']

client = AzureOpenAI(
    api_key= AZURE_OPENAI_API_KEY,
    api_version=OPENAI_API_VERSION,
    azure_endpoint= AZURE_OPENAI_ENDPOINT,
)


# get_section, has_multiple_lines, extract_requirements functions are defined in helpers2.py
# def get_section(file_content, start_pattern, end_pattern, include_start_pattern=True):
    # This function takes a string "file_content" and returns the substring starting with the start_pattern and
    # ending with the end_pattern.   If the start_pattern and the end pattern  are not found, it returns
    # the empty string.  If the start_pattern is not found but the end_pattern is found, then it returns the substring
    # from the beginning until the end_pattern.  If the start_pattern is found but not the end_pattern, it returns the
    # substring from the start_pattern until the end of the string.
    # print(f"file_content = \n{file_content}, \nstart_pattern = '{start_pattern}', end_pattern = '{end_pattern}'")
    # print(f"get_section: start_pattern = {start_pattern}")
# def has_multiple_lines(file_path):
#   """Checks if a file has more than one line.
# def generate_requirements_file(program_string, dir):
#     """Generates a requirements.txt file for the program program_string and stores it in directory dir.


# state that is passed from node to node of graph
class GraphState(TypedDict):
    service_name: str             # name of service
    docker_container_name:str     # name of docker container running this microservice
    mongo_container_name: str     # name of docker container for mongoDB
    docker_image_name:str         # name of image used to create container for this service
    sample_num: int               # as we may experiment multiple times, we give each sample its own number. initialized
                                  # before calling graphInit.
    base_dir: str                 # base directory where ground truth (GT) services reside
    gen_dir: str                  # subdirectory of base_dir where the generated code and artifacts are put
    test_prog: str                # the full path name of the program that executes tests on the microservices
    initial_user_prompt: str      # initial user prompt for generating this microservice code
    initial_sys_prompt: str       # "" system prompt ""
    ms_code: str                  # last microservice code generated
    hasRTerrs: bool               # boolean indicating whether there were any runtime Python errors when executing tests
    hasTestErrs: bool             # boolean indicating whether any microservice tests failed
    errors: str                   # the run-time errors from current ms_code
    reflection: str               # reflection on the nature of the error in the program
    num_retries: int              # number of times retried to generate working microservice code


# agent to initialize GraphState.   note that service_name is initialized before entering this agent
def graphInit(state):
    print("** Entering Agent GraphInit **")
    # base_dir is the base directory where the service GT code and dockerfile are stored.
    # gen_dir = base_dir/timestamp&model will be the directory to store the generated services.
    # test_prog is the python script that tests the microservices
    # state['base_dir'], state['gen_dir'], state["service_name"], state["sample_num"] are initialized before invoking
    # this function.
    # create initial user prompt
    service_specific_prompt_file = state["base_dir"]  + "prompts/prompt_" + state["service_name"] + ".txt"
    f = open(service_specific_prompt_file)
    service_specific_prompt = f.read()
    f.close()
    state["initial_sys_prompt"] = {"role": "system", "content": "You are an expert in generating Python microservices "
                                                                "using REST APIs"}
    state["initial_user_prompt"] = {"role": "user", "content": service_specific_prompt}
    # set location of test script
    state['test_prog'] = "/Users/danielyellin/PycharmProjects/stocks/tests/unitTests.py"
    state['ms_code'] = ''           # no microservice code yet
    state['hasRTerrs'] = False      # boolean indicating whether there were any runtime errors when executing tests
    state['hasTestErrs'] = False    # boolean indicating whether any microservice tests failed
    state['errors'] = ''            # the run-time and testing errors from current ms_code
    state['reflection'] = ''        # reflection generated by LLM when given errors in generated code
    state['num_retries'] = 0        # have not generated any code yet
    return state


def generateMicroservice(state):
    print("** Entering Agent GenerateMicroservice **")
    # base_dir is the directory to store the generated code
    service_name = state["service_name"]
    sample_num = str(state["sample_num"])
    version_num = str(state['num_retries'])
    msg = [state["initial_user_prompt"], state["initial_sys_prompt"]]
    # use model='gpt-35-16k' or model='gpt4' given in MODEL variable, generate a single sample (n=1), use MAX_TOKENS_1
    responses = client.chat.completions.create(messages = msg, model=MODEL, temperature= TEMP,
                                               max_tokens=MAX_TOKENS_1, n = 1)
    print("MAX_TOKENS_1 requests = ", MAX_TOKENS_1)
    print(f"Total tokens used = {responses.usage.total_tokens}. Prompt tokens = {responses.usage.prompt_tokens}, "
          f"Completion_tokens = {responses.usage.completion_tokens}")
    meta_fn.write(f"Agent GenerateMicroservice\n"
                  f"   service = {service_name}\n"
                  f"   MAX_TOKENS_1 requests =  {MAX_TOKENS_1}\n"
                  f"   Total tokens used = {responses.usage.total_tokens}\n"
                  f"   Prompt tokens = {responses.usage.prompt_tokens}, "
                  f"   Completion_tokens = {responses.usage.completion_tokens}\n"
                  f"   Temperature = {TEMP}\n\n"
                  )
    ms_code = responses.choices[0].message.content
    # remove leading "```python" until "```" if found.
    # if trailing 3 single quotation marks are missing, should still remove initial `"``python".
    # last parameter is False so do not include start_pattern (```python) in returned code.
    ms_clean_code= get_section(ms_code, "```python", "```", False)
    # if do not find the previous prefix and suffix, then use the generated code.
    if ms_clean_code == '':
        ms_clean_code = ms_code
    # initial code generated will be of the form <ms-name>-s<num>-v<sample_num>.py
    f = open(state["gen_dir"] + service_name + "-s" + sample_num + "-v" + version_num + ".py", "w+")
    print(f"writing generated code to file "
          f"{state['gen_dir'] + service_name + '-s' + sample_num + '-v' + version_num + '.py'}")
    f.write(ms_clean_code)
    f.close()
    state['ms_code'] = ms_clean_code
    return state


def runDockerAndTest(state):
    print("** Entering Agent RunDockerAndTest **")
    # (i) copy files to set up docker compose.  it will run the LLM-generated code for the microservice and the
    # ground truth for all the other services (if there are other services).
    # (ii) execute docker-compose to run the microservices.
    # (iii) run the tests on the microservices.  The tests are in the python script test_fn
    # (iv) store the run-time errors during testing. the test results are stored by the test script.
    # (v) remove docker containers
    state['errors'] = '' # reset as no errors yet for this code
    state['hasTestErrs'] = False
    state['hasRTerrs'] = False
    base_dir = state['base_dir']    # this is the directory that holds the GT code and dockerfile for each service
    gen_dir = state['gen_dir']      # this is the directory that stores the generated code
    service_name = state['service_name']
    # log_fn_prefix is the filename of the ms code, without the ".py" extension.   We will use this name to generate
    # other files -- e.g., log files describing errors found when running the ms code
    log_fn_prefix = state['service_name'] + "-s" + str(state['sample_num']) + "-v" + str(state['num_retries'])
    gen_service_prefix = gen_dir + log_fn_prefix  # gen_service_prefix with the extension ".py" is the generated code
    print(f"base_dir = {base_dir} and gen_dir = {gen_dir} and service_name = {service_name}")

    # i.  copy the code for the generated service to the appropriate directory.  the generated code for service S has
    # the name "S-vn.py", where n = version num
    # **GENERALIZE**   the directory structure below: {service_name}/{service_name}.py
    shutil.copyfile(gen_service_prefix + ".py", base_dir + f"{service_name}/{service_name}.py")
    # generate requirements file for generated code and copy it to write directory
    try:
        h = open(gen_service_prefix + ".py", "r")
        prog = h.read()
        h.close()
        # the following line generates requirements.txt for prog and store it in directory base_dir + service_name
        generate_requirements_file(prog, base_dir + service_name)
    except Exception as e:
        print("************ Exception when generating requiorements ************ ")
        print("exception = ", e)


    # ii. remove old images and containers then run docker compose so that the services are running
    print("************ Removing any existing Docker microservice images (except mongo) and containers ************")
    ### **GENERALIZE: NEED TO FIX THIS IF RUNNINIG MULTIPLE SERVICES **
    # list_images()
    # for service in SERVICES:
    #     print("attempting to delete image mystate['docker_image_name")
    remove_image(mystate['docker_image_name'])
    print(f"images remaining after trying to remove {mystate['docker_image_name']}")
    list_images()
    print("existing containers: ")
    # list_containers()
    print("attempting to delete containers mystate['docker_container_name']' and mystate['mongo_container_name'] ")
    # we remove all existing containers (not just the two stated above) to avoid naming issues.
    remove_all_containers()
    print("containers remaining after trying to delete containers")
    list_containers()
    print("************ Setting up containers using Docker Compose ************")
    try:
        # IMPORTANT NOTE.   This docker compose up cmd works correctly this cmd is run in the same directory that
        # contains the docker-compose.yml.  If you run it in a different directory (using -f parameter) as I tried in
        # the next line, it leaves a dangling container when issuing docker compose down, thereby also leaving the
        # mongo container running, and also leaving all images intact.   Therefore, we put the docker-compose.yml
        # in this directory.
        # output1 = subprocess.run(["docker-compose", "-f", "../docker-compose.yml", "up", "--build", "-d"],
        #                       capture_output=True, text=True)
        output1 = subprocess.run(["docker-compose", "up", "--build", "-d"], capture_output=True, text=True)
    except Exception as e:
        print("************ Exception when executing setting up Docker containers ************ ")
        print("exception = ", e)
        # print("Executing this process produced the following error messages:\n", output1.stderr)
    print("output1.returncode= ", str(output1.returncode))
    print("output1.stderr= ", output1.stderr)
    print("output1.stdout = ", output1.stdout)
    if "pip install" in output1.stderr:
        # then cannot install dependencies.   write to error file and exit
        fh = open(gen_service_prefix + "-runtime-errors.txt", "w")
        fh.write(output1.stderr)
        fh.close()
        print("output1.stdout= ", output1.stdout)
        exit("Dockerfile had issue with pip install")
    # ii-b.  test that all the services are started
    loopnum = 0
    upAndRunnning = False
    while loopnum < 2 and not upAndRunnning:
        # Example usage (assuming 'docker' library is installed)
        container_name = mystate['docker_container_name']
        if is_container_running(container_name):
            upAndRunnning = True
        else:
            print(f"Container '{container_name}' is not running")
        loopnum = loopnum + 1
        time.sleep(1)   # sleep for 1 second to give time for container to start

    # iii.Run tests on the microservices.
    try:
        print("************ Executing Tests ************")
        # run test program.   get error messages from stderr.   remove error messages about OpenSSL and warnings.
        # 1st parm (gen_dir) is directory to write output to.
        # 2nd parm (log_fn_prefix) is the name of the ms file without the extension.
        output2 = subprocess.run(["python", state['test_prog'], gen_dir, log_fn_prefix, "-d"],
                                 capture_output=True, text=True)
    except Exception as e:
        print("************ Exception when executing tests ************ ")
        print("exception = ", e)
    print("output2.returncode= ", str(output2.returncode))
    print("output2.stdout= ", output2.stdout)
    print("output2.stderr= ", output2.stderr)
    # iv.  retrieve the docker logs (output3logs) and store the results.  Note that:
    #      output1.stderr = error messages from the microservice containers
    #      output2.stdout = messages that the test script on execution prints to stdout
    #      output2.stderr = any error messages that python execution sends to stderr when executing the test script.
    #                       Usually this is just a warning the error messages (e.g., "NotOpenSSLWarning")
    #      output3.stdout = output printed by docker logs to stdout and stderr.  Usually just message a couple of
    #                       standard messages such as "* Serving Flask app 'stocks.py' * Debug mode: on" and any output
    #                       the program itself prints out.
    #      output3.stderr = error messages from the microservices containers' logs.  This will contain the requests that
    #                       failed: "192.168.65.1 - - [25/Nov/2024 05:51:38] "POST /stocks HTTP/1.1" 400" but (not sure
    #                       why) can contain also successful messages:
    #                       "192.168.65.1 - - [25/Nov/2024 05:51:38] "GET /stocks HTTP/1.1" 200 -".
    #      output4.stderr = error messages from the docker compose down command.
    print("output2.stdout = ", output2.stdout)
    # unitTestsOutput = output2.stdout
    print("*********************************************************************************")
    print("output2.stderr = ", output2.stderr)
    print("*********************************************************************************")
    TracebackOutput2 = extract_errors(get_section(output2.stdout, start_pattern="Traceback", end_pattern=
                                                  "no more error messages", include_start_pattern=True),service_name)
    try:
        print("************ Retrieving docker logs ************ ")
        output3 = subprocess.run(["docker", "logs", container_name], capture_output=True, text=True)
        print("output3.stdout = ", output3.stdout)
        print("*********************************************************************************")
        print("output3.stderr = ", output3.stderr)
        print("*********************************************************************************")
        # remove the first part of the logs that are not error messages.  the end_pattern is random and this call will
        # therefore just truncate unneeded header info.
        output3logs = get_section(output3.stderr, start_pattern="Traceback", end_pattern="no more error messages",
                                  include_start_pattern=True)
        #   a. collect the output from the unit tests (on stdout).   extract the output starting from the keyword
        #      "Traceback". then "compress" these error messages (as it can be voluminous and contains information that
        #      is hard to use for debugging).  The compression finds lines containing "S.py", where S is the name of the
        #      service being generated (we assume the code being tested is in a file of that name). It will take the line
        #      containing "S.py" and the next 4 lines, as usually that suffices to describe the error.
        #   b. collect the docker log files from the docker container for S.    follow the same process as in (a) to
        #      collect and compress error messages.
        #      **** NEED TO CHECK that there is not overlap between (a) and (b)  ********
        #   c. append the error messages from (b) to those from (a) to form the runtime error messages RT. if RT is
        #      empty,there are no RT error messages.  If it is not empty, then set the error messages E to be RT.
        #   d. if there are not any RT errors, then assuming there are test errors TE (report from the unit tests on
        #      failed tests), set E to TE together with the direct output from the unit tests (the output it prints to
        #      stdout)
        # get_section will return '' if start and end patterns not found.  If "Traceback" not found, then output3logs
        # will equal '' (the empty string).
        print("output3.returncode= ", str(output3.returncode))
        print("output3logs =", output3logs)
        TracebackOutput3 = extract_errors(output3logs, service_name)
        tracebackErrs = TracebackOutput2 + TracebackOutput3
        print("tracebackErrs = ", tracebackErrs)
        if tracebackErrs == '':  # no runtime errors
            state['hasRTerrs'] = False
            print('no runtime errors')
        else:
            state['hasRTerrs'] = True
            print('has runtime errors')
        if state['hasRTerrs']:
            # if Python runtime errors, then set state['errors'] to these error messages
            state['errors'] = tracebackErrs
            fh = open(gen_service_prefix + "-runtime-errors.txt", "w")
            fh.write(tracebackErrs)
            fh.close()
        state['hasTestErrs'] = has_multiple_lines(gen_service_prefix + "-test-errors.txt")
        if state['hasTestErrs']:
            f = open(gen_service_prefix + "-test-errors.txt", "r")
            test_errors = f.read()
            f.close()
            state['errors'] = state['errors'] + test_errors  # + unitTestsOutput
            # print("no RT error but has test errors = ")
            print(f"state['errors'] = {state['errors']}")
    except Exception as e:
        print("************ Exception when executing tests ************ ")
        print("exception = ", e)

    # v. kill and remove the containers
    try:
        print("************ Removing containers ************ ")
        print("images before docker compose down call")
        list_images()
        print("containers before docker compose down call")
        list_containers()
        output4 = subprocess.run(["docker-compose", "down", "--rmi", "all", "-v"], capture_output=True, text=True)
        print("output4.stderr = ", output4.stderr)
        print("output4.stdout = ", output4.stdout)
        print("output4.returncode= ", output4.returncode)
        print("images after docker compose down call")
        list_images()
        print("containers after docker compose down call")
        list_containers()
    except Exception as e:
        print("************ exception raised in removing containers ************")
        print(e)
        print("stderr = ", output4.stderr)
    return state


# Can add strategy that if # testing errors is decreasing, continue to try to correct code further
def chk4rErr(state):
    print("** Entering Agent chk4rErr **")
    if (state['hasRTerrs'] or state['hasTestErrs']):
        if state['num_retries'] < MAX_RETRIES:
            return "ReflectOnErr"
        else:
            print(f"Generated code {MAX_RETRIES} number of times and still have errors")
            return "FinalizeOut"
    else:
        print(f"Generated code without errors")
        meta_fn.close()
        return "END"


def reflectOnErr(state):
    print("** Entering Agent ReflectOnErr **")
    error_messages = state['errors']
    f = open(state['base_dir'] + "prompts/prompt_reflect.txt")
    prompt_reflect = f.read()
    f.close()
    # added the following lines to shorten error message becaues for gpt3.5 was getting the following error message:
    # openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16384
    # tokens. However, you requested 23319 tokens (7319 in the messages, 16000 in the completion). Please reduce the
    # length of the messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code':
    # 'context_length_exceeded'}}
    original_error_length = len(error_messages)
    shorten = False
    if GPT35:
        if original_error_length > 5000:
            error_messages = error_messages[0:5000]
            shorten = True
    else:  # GPT4, have less tokens
        if original_error_length > 4000:
            error_messages = error_messages[0:4000]
            shorten = True
    system_msg = {"role": "system",
                  "content": "The Python Interpreter tried to execute the following program: " + state['ms_code']
                             + " There errors were found during execution: " + error_messages}
    user_msg = {"role": "user",
                  "content": prompt_reflect}
    messages = [state['initial_user_prompt']]
    messages.append(system_msg)
    messages.append(user_msg)
    print("reflect messages = ", messages)
    response = client.chat.completions.create(messages=messages, model=MODEL, temperature=TEMP, max_tokens=MAX_TOKENS_2,
                                              n=1)
    print(" MAX_TOKENS_2 requests = ", MAX_TOKENS_2)
    print(f"Total tokens used = {response.usage.total_tokens}. Prompt tokens = {response.usage.prompt_tokens}, "
          f"Completion_tokens = {response.usage.completion_tokens}")
    meta_fn.write(f"Agent reflectOnErr\n"
                  f"   Length of original error_massage was {original_error_length}.   "
                  f"   Shorten = {str(shorten)}\n"
                  f"   MAX_TOKENS_1 requests =  {MAX_TOKENS_2}\n"
                  f"   Total tokens used = {response.usage.total_tokens}\n"
                  f"   Prompt tokens = {response.usage.prompt_tokens}, Completion_tokens = {response.usage.completion_tokens}\n"
                  f"   Temperature = {TEMP}\n\n"
                  )
    state['reflection'] = response.choices[0].message.content
    base_filename = state["gen_dir"] + state['service_name'] + "-s" + str(state['sample_num']) + "-v" + \
                          str(state['num_retries'])
    f = open(base_filename + "-reflect.txt", "w")
    f.write(state['reflection'])
    f.close()
    return state


def reGenPgm(state):
    print("** Entering Agent reGenPgm **")
    print("num_retries = ", state['num_retries'])
    system_msg2 = {"role": "system", "content": state['ms_code']}
    # error_message = state['errors']    # too long and gertting context too long error message
    reflection = state['reflection']
    reflection_content = f"When the Python Interpreter tried to execute this program,there were errors.   Here is the "\
                         f"summary and instructions on how to fix these errors: {reflection}.  Re-generate the code" \
                         f"and make sure to fix the errors. Do ot use CustomJSONEncoder.  Check that you have fixed"  \
                         f"each error."
    human_msg2 = {"role": "user",
                  "content": reflection_content}
    messages = [system_msg2,human_msg2]
    response = client.chat.completions.create(messages=messages, model=MODEL, temperature=TEMP, max_tokens=MAX_TOKENS_1,
                                              n=1)
    print(" MAX_TOKENS_1 requests = ", MAX_TOKENS_1)
    print(f"Total tokens used = {response.usage.total_tokens}. Prompt tokens = {response.usage.prompt_tokens}, "
          f"Completion_tokens = {response.usage.completion_tokens}")
    meta_fn.write(f"Agent reGenPgm\n"
                  f"   MAX_TOKENS_1 requests =  {MAX_TOKENS_1}\n"
                  f"   Total tokens used = {response.usage.total_tokens}\n"
                  f"   Prompt tokens = {response.usage.prompt_tokens}, Completion_tokens = \
                                       {response.usage.completion_tokens}\n"
                  f"   Temperature = {TEMP}\n\n"
                  )
    generated_code = response.choices[0].message.content
    # print("generated code = ", generated_code)
    state['num_retries'] += 1
    # last parameter is False so do not include start_pattern in returned code
    ms_clean_code = get_section(generated_code, "```python", "```", False)
    # if does not have "```python" prefix, then use raw generated code
    if ms_clean_code == '':
        ms_clean_code = generated_code
    base_filename = state["gen_dir"] + state['service_name'] + "-s" + str(state['sample_num']) + "-v" \
                    + str(state['num_retries'])
    f = open(base_filename + ".py", "w+")
    print(f"writing re-generated code to file {base_filename + '.py'}")
    f.write(ms_clean_code)
    f.close()
    state['ms_code'] = ms_clean_code
    # state['chat_history'] = state['chat_h  istory'].append({"role": "system", "content": state['ms_code']})
    return state


# FinalizeOut agent is entered if we are terminating the program but we still have errors.
def finalizeOut(state):
    print("** Entering Agent FinalizeOut **")
    error_messages = state['errors']
    f = open(state['base_dir'] + "prompts/prompt_reflect.txt")
    prompt_reflect = f.read()
    f.close()
    # added the following lines to shorten error message becaues for gpt3.5 was getting the following error message:
    # openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16384
    # tokens. However, you requested 23319 tokens (7319 in the messages, 16000 in the completion). Please reduce the
    # length of the messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code':
    # 'context_length_exceeded'}}
    original_error_length = len(error_messages)
    shorten = False
    if GPT35:
        if original_error_length > 5000:
            error_messages = error_messages[0:5000]
            shorten = True
    else:  # GPT4, have less tokens
        if original_error_length > 4000:
            error_messages = error_messages[0:4000]
            shorten = True
    system_msg = {"role": "system",
                  "content": "The Python Interpreter tried to execute the following program: " + state['ms_code']
                             + "There errors were found during execution: " + error_messages}
    human_msg = {"role": "user",
                 "content": prompt_reflect}
    messages = [state['initial_user_prompt'], state['initial_sys_prompt']]
    messages.append(system_msg)
    messages.append(human_msg)
    print("messages = ", messages)
    response = client.chat.completions.create(messages=messages, model=MODEL, temperature=TEMP, max_tokens=MAX_TOKENS_2,
                                              n=1)
    print(" MAX_TOKENS_2 requests = ", MAX_TOKENS_2)
    print(f"Total tokens used = {response.usage.total_tokens}. Prompt tokens = {response.usage.prompt_tokens}, "
          f"Completion_tokens = {response.usage.completion_tokens}")
    print(" MAX_TOKENS_2 requests = ", MAX_TOKENS_2)
    meta_fn.write(f"Agent finalizeOut\n"
                  f"   Length of original error_massage was {original_error_length}.   "
                  f"   Shorten = {str(shorten)}\n"
                  f"   MAX_TOKENS_1 requests =  {MAX_TOKENS_2}\n"
                  f"   Total tokens used = {response.usage.total_tokens}\n"
                  f"   Prompt tokens = {response.usage.prompt_tokens}, Completion_tokens = {response.usage.completion_tokens}\n"
                  f"   Temperature = {TEMP}\n\n"
                  )
    meta_fn.close()
    state['reflection'] = response.choices[0].message.content
    base_filename = state["gen_dir"] + state['service_name'] + "-s" + str(state['sample_num']) + "-v" + str(state['num_retries'])
    f = open(base_filename + "-reflect.txt", "w")
    f.write(state['reflection'])
    f.close()
    return state


# ---------   start main program    ---------#
# build langgraph graph
workflow = StateGraph(GraphState)
workflow.add_node("GraphInit", graphInit)
workflow.add_node("GenerateMicroservice",generateMicroservice)
workflow.add_node("RunDockerAndTest", runDockerAndTest)
workflow.add_node("ReflectOnErr",reflectOnErr)
workflow.add_node("ReGenPgm",reGenPgm)
workflow.add_node("FinalizeOut",finalizeOut)
workflow.set_entry_point("GraphInit")
# add edges
workflow.add_edge("GraphInit", "GenerateMicroservice")
workflow.add_edge("GenerateMicroservice", "RunDockerAndTest")
workflow.add_conditional_edges("RunDockerAndTest",chk4rErr)
workflow.add_edge("ReflectOnErr", "ReGenPgm")
workflow.add_edge("ReGenPgm","RunDockerAndTest")
workflow.add_edge("FinalizeOut", END)
# compile and run
runnable = workflow.compile()
mystate = GraphState()
d = datetime.now()
curtime = d.strftime("%d-%m-%Y?-%M%S")
base_dir = "/Users/danielyellin/PycharmProjects/stocks/"
gen_dir =   base_dir + curtime + '-' + MODEL + '-temp' + str(TEMP) + '-1tok' + str(MAX_TOKENS_1) + '-2tok' + str(MAX_TOKENS_2)  + '/'
os.mkdir(gen_dir)
meta_file = gen_dir  + '/meta_data'
meta_fn = open(meta_file, 'w+')
printdate = d.strftime("%d-%m-%Y-%H-%M")
meta_fn.write(f"{printdate} Hours-Minutes\n"
        f"Model = {MODEL}\n"
        f"MAX_TOKENS_1 = {MAX_TOKENS_1}\n"
        f"MAX_TOKENS_2 = {MAX_TOKENS_2}\n"
        f"Temperature = {TEMP}\n"
)
for service_name in SERVICES:
    mystate['base_dir'] = base_dir
    mystate['gen_dir'] = gen_dir
    mystate["service_name"] = service_name
    mystate["sample_num"] = 1
    # getting the docker container name is important in order to check if ms container is running and then to remove it
    # when doen testing.   In this case, we invoke docker-compose from base_dir.  The Dockerfile for the ms is in the
    # "stocks" subdirectory of bse_dir.  The docker-compose.yml gives this service the name "portfolio".   We are only
    # running a single instance of this container.  Hence the name is '/stocks-portfolio-1'.  It turns out that you can
    # name containers in docker-compose.https://forums.docker.com/t/custom-container-name-for-docker-compose/48089/2
    # Hence in the docker-compose file i set the name to "my-stock-service".  Note that eventhough the docker-compose
    # file sets the name to "my-stock-service", docker gives it the name "/my-stock-service"
    # mystate['docker_container_name'] = '/stocks-portfolio-1'
    mystate['docker_container_name'] = "/my-stock-service"
    mystate['mongo_container_name'] = "/my-mongo-service"
    mystate['docker_image_name'] = 'stocks-portfolio:latest'
    print("base_dir = ", base_dir)
    print("gen_dir = ", gen_dir)
    print("service_name = ", service_name)
    print('docker_container_name = ', '/my-stock-service')
    runnable.invoke(mystate)
meta_fn.close()


