
# This program iteratively generates each microservice (one at a time, total 4) comprising a public library application.
# Each iteration will test the application using the generated microservice and using the ground truth (GT)
# implementations of the 3 other microservices, effectively testing just the generated microservice.
# The program uses the LangGraph framework.
# For each iteration (generating one of the microservices S):
# 1. Invoke graphInit to initialize graph state,including initial prompt
# 2. Invoke generate_microservice to synthesize the S microservice code per requirements and code generation guidelines.
#    Store the generated microservice in a file.
# 3. Invoke run_docker_and_tests:
#    (a) copies the generated code for S to the appropriate directory, It also finds the requirements for S and
#    generates a requirements.txt file for the Dockefile to use.
#    (b) sets up docker containers to run the code generated for S, and the ground truth code for the other
#    services.   It uses docker-compose.
#    (c) tests the system using the public library unit tests. (d) records any errors
#    in the docker log for service S, and
#    (e) shuts down the containers and removes them.
# 4. Execute agent Chk4rErr.   If there are testing errors and the docker log file has errors and have not reached
#    MAX_RETRIES, then go to 5. Otherwise, if no errors or have errors and MAX_RETRIES reached, go to (7).
# 5  Execute agent ReflectOnErr.   Reflect on errors in the docker log file.  Add reflection response to prompt for S.
# 6. Using revised prompt, execute agent ReGenQueryPgm, generating a new program taking the errors and reflection into
#    consideration. Goto (3).
# 7. Execute agent FinalizeOut.   Finalize outputs and go to END.
#
# Output of program is 5 files.   These are stored in the subdirectory <timestamp>-<model>.
# Example: "16-08-2024?-4852-gpt-35-16k"
# a. <service-name>-s<n1>-v<n2>.py is the generated code.   There can be more than one file, as n1 is the sample number
#    of the generated code (we are using s=1 for now), and n2 ranges from 0 to MAX_RETRIES;  I.e., for n2 > 0, it is the
#    regenerated code when version n2-1 had errors and we have not yet reached MAX_RETRIES.
# b. <service-name>-s<n1>-v<n2>-runtime-errors.txt is the docker log containing Python Traceback errors for the
#    generated program <service-name>-s<n1>-v<n2>.py.
# c. <service-name>-s<n1>-v<n2>-test-errors.txt is the list of failed unit tests for <service-name>-s<n1>-v<n2>.py with
#    a message explaining the failure.  This file is generated by the unit test script.
# d. <service-name>-s<n1>-v<n2>-test-log.txt is the log file providing more details on which tests passed or failed and
#    generated statistics.  This file is generated by the unit test script.
# e. <service-name>-s<n1>-v<n2>-reflect.txt is the generated reflection the describes the cause of the errors in
#    <service-name>-s<n1>-v<n2>-runtime-errors.txt


import shutil

import time
from helpers import get_section, has_multiple_lines, extract_errors, generate_requirements_file
from docker_funcs import list_containers, is_container_running, remove_all_containers, list_images, remove_image
from openai import AzureOpenAI
from datetime import datetime
from typing import TypedDict
from langgraph.graph import StateGraph, END
import os
import subprocess

# --------- Initialize variables controlling model, temperature, tokens, number of retries, and microservice --------#
# THESE variables need to be set according to experiment being performed.
# Set up GPT4 model
AZURE_OPENAI_API_KEY = os.getenv('SUBSCRIPTION_OPENAI_API_KEY')
AZURE_OPENAI_ENDPOINT = os.getenv('SUBSCRIPTION_OPENAI_ENDPOINT')
OPENAI_API_VERSION = '2023-12-01-preview'
# GPT35 = True
GPT35 = False   # if not GPT35 then GPT4
TEMP = 0
# TEMP = .3
# TEMP = .5

# Choose model to use
# MODEL = 'gpt-35-16k'   # Model Name: gpt-35-turbo-16k.   Modified on: Apr 15, 2024 4:17 PM    Max Tokens: 16,384
# MAX_TOKENS = 10000
# the following was error message for gpt-35-16k when i set max-tokens too high - you cannot set it to 16k
#   openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16384 tokens.
# However, you requested 18183 tokens (2183 in the messages, 16000 in the completion). Please reduce the length of the
# messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
#   openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16384 tokens.
# However, you requested 16620 tokens (4620 in the messages, 12000 in the completion). Please reduce the length of the
# messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
# similarly for gpt4 - more than 6000 problematic.   Here is error message when I set it to 7500:
#   openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens.
# However, you requested 9649 tokens (2149 in the messages, 7500 in the completion). Please reduce the length of the
# messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
# MODEL = 'gpt4'       # Model Name: gpt-4.              Modified on: May 15, 2024 3:45 PM.   Max Tokens:  8,192 tokens.
# gpt-4-0613: gpt-4 model with a context window of 8,192 tokens. Training data up to September 2021.
#   openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens.
# However, you requested 8637 tokens (2637 in the messages, 6000 in the completion). Please reduce the length of the
# messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
# even MAX_TOKENS 4100 was too much for reflection
# can set it to 6000 for generating code and less for reflect invocation/regen invocation.   or shorten prompt
MAX_TOKENS_GPT35_1 = 7000
MAX_TOKENS_GPT35_2 = 6000
MAX_TOKENS_GPT4_1 = 5000
MAX_TOKENS_GPT4_2 = 3000
if GPT35:
    MODEL = 'gpt-35-16k'
    MAX_TOKENS_1 = MAX_TOKENS_GPT35_1
    MAX_TOKENS_2 = MAX_TOKENS_GPT35_2
else:   # using GPT4 with less tokens
    MODEL = 'gpt4'
    MAX_TOKENS_1 = MAX_TOKENS_GPT4_1
    MAX_TOKENS_2 = MAX_TOKENS_GPT4_2

NUM_SAMPLES = 1
MAX_RETRIES = 2  # max number of times to retry to generate error-free program
SERVICES = ['cardholders','books','borrows','logs']
BASE_DIR = ??? "/Users/danielyellin/PycharmProjects/stocks/" /Users/danielyellin/PycharmProjects/RestGen/automation/generate/"
GEN_DIR_PREFIX = "TestsForPaper/"
TEST_PROG = "/Users/danielyellin/PycharmProjects/stocks/tests/unitTests.py"
DOCKER_CONTAINER_NAME = "/???"  "/my-stock-service"
MONGO_CONTAINER_NAME = "/my-mongo-service"
DOCKER_IMAGE_NAME = '??'  'stocks-portfolio:latest'
REGEN_JUST_ERRS = True   # When True then when re-generating code, only supply the error code and the error messages
# REGEN_JUST_ERRS = False  # When False, also supply the microservice description.
INCLUDE_EX_CODE = True       # When True, supplying example Python, Flask, MongoDB code as "template"
# INCLUDE_EX_CODE = False

client = AzureOpenAI(
    api_key= AZURE_OPENAI_API_KEY,
    api_version=OPENAI_API_VERSION,
    azure_endpoint= AZURE_OPENAI_ENDPOINT,
)


# get_section and has_multiple_lines defined in helpers
# def get_section(file_content, start_pattern, end_pattern, include_start_pattern=True):
    # This function takes a string "file_content" and returns the substring starting with the start_pattern and
    # ending with the end_pattern.   If the start_pattern and the end pattern  are not found, it returns
    # the empty string.  If the start_pattern is not found but the end_pattern is found, then it returns the substring
    # from the beginning until the end_pattern.  If the start_pattern is found but not the end_pattern, it returns the
    # substring from the start_pattern until the end of the string.
    # print(f"file_content = \n{file_content}, \nstart_pattern = '{start_pattern}', end_pattern = '{end_pattern}'")
    # print(f"get_section: start_pattern = {start_pattern}")


# def has_multiple_lines(file_path):
#   """Checks if a file has more than one line.
# def generate_requirements_file(program_string, dir):
#     """Generates a requirements.txt file for the program program_string and stores it in directory dir.


# state that is passed from node to node of graph
class GraphState(TypedDict):
    service_name: str               # one of "cardholders", "books", "borrows", or "logs".  initialized before calling graphInit
    # docker_container_name:str     # name of docker container running this microservice
    # mongo_container_name: str     # name of docker container for mongoDB
    # docker_image_name:str         # name of image used to create container for this service
    sample_num: int               # as we may experiment multiple times, we give each sample its own number. initialized
                                  # before calling graphInit.  Since we currently generate just one sample per LLM call,
                                  # not significant currently
    base_dir: str                   # base directory where ground truth (GT) services reside
    gen_dir: str                    # subdirectory of base_dir where the generated code and artifacts are put
    test_prog: str                  # the full path name of the program that executes tests on the microservices
    initial_user_prompt: str        # initial user prompt for generating this microservice code
    initial_sys_prompt: str         # "" system prompt ""
    ms_code: str                    # last microservice code generated
    hasRTerrs: bool                 # boolean indicating whether there were any runtime Python errors when executing tests
    hasTestErrs: bool               # boolean indicating whether any microservice tests failed
    errors: str                     # the run-time errors from current ms_code
    reflection: str                 # reflection on the nature of the error in the program
    num_retries: int                # number of times retried to generate working microservice code


# agent to initialize GraphState.   note that service_name is initialized before entering this agent
def graphInit(state):
    print("** Entering Agent GraphInit **")
    # base_dir is the base directory where the cardholders, books, borrows and logs GT code and dockerfiles are stored.
    # gen_dir = base_dir/timestamp&model will be the directory to store the generated services.
    # test_prog is the python script that tests the microservices
    # state['base_dir'], state['gen_dir'], state["service_name"], state["sample_num"] are initialized before invoking
    # this function.
    # create initial user prompt
    f = open(state["base_dir"] + "prompts/prompt_addendum")
    addendum = f.read()
    f.close()
    service_specific_prompt_file = state["base_dir"]  + "prompts/prompt_" + state["service_name"] + ".txt"
    f = open(service_specific_prompt_file)
    service_specific_prompt = f.read()
    state["initial_sys_prompt"] = {"role": "system", "content": "You are an expert in generating Python microservices using REST APIs"}
    state["initial_user_prompt"] = {"role": "user", "content": service_specific_prompt + addendum}
    f.close()
    if INCLUDE_EX_CODE:
        example_promppt_file = state["base_dir"] + "prompts/prompt_" + "example" + ".txt"
        f = open(example_promppt_file)
        example = f.read()
        f.close()
        service_specific_prompt = service_specific_prompt + example
    state["initial_sys_prompt"] = {"role": "system", "content": "You are an expert in generating Python microservices "
                                                                "using REST APIs"}
    state["initial_user_prompt"] = {"role": "user", "content": service_specific_prompt}
    # set location of test script
    state['test_prog'] = TEST_PROG
    state['ms_code'] = ''           # no microservice code yet
    state['hasRTerrs'] = False      # boolean indicating whether there were any runtime errors when executing tests
    state['hasTestErrs'] = False    # boolean indicating whether any microservice tests failed
    state['errors'] = ''            # the run-time and testing errors from current ms_code
    state['reflection'] = ''        # reflection generated by LLM when given errors in generated code
    state['num_retries'] = 0        # have not generated any code yet
    return state


def generateMicroservice(state):
    print("** Entering Agent GenerateMicroservice **")
    # base_dir is the directory to store the generated code
    service_name = state["service_name"]
    sample_num = str(state["sample_num"])
    version_num = str(state['num_retries'])
    msg = [state["initial_user_prompt"], state["initial_sys_prompt"]]
    # use model='gpt-35-16k' or model='gpt4' given in MODEL variable, generate a single sample (n=1), use MAX_TOKENS_1
    responses = client.chat.completions.create(messages = msg, model=MODEL, temperature= TEMP,
                                               max_tokens=MAX_TOKENS_1, n = 1)
    print("MAX_TOKENS_1 requests = ", MAX_TOKENS_1)
    print(f"Total tokens used = {responses.usage.total_tokens}. Prompt tokens = {responses.usage.prompt_tokens}, "
          f"Completion_tokens = {responses.usage.completion_tokens}")
    meta_fn.write(f"Agent GenerateMicroservice\n"
                  f"   service = {service_name}\n"
                  f"   MAX_TOKENS_1 requests =  {MAX_TOKENS_1}\n"
                  f"   Total tokens used = {responses.usage.total_tokens}\n"
                  f"   Prompt tokens = {responses.usage.prompt_tokens}, Completion_tokens = {responses.usage.completion_tokens}\n"
                  f"   Temperature = {TEMP}\n\n"
                  )

    ms_code = responses.choices[0].message.content
    # remove leading "```python" until "```" if found.
    # if trailing 3 single quotation marks are missing, should still remove initial `"``python".
    # last parameter is False so do not include start_pattern (```python) in returned code.
    ms_clean_code= get_section(ms_code, "```python", "```", False)
    # if do not find the previous prefix and suffix, then use the generated code.
    if ms_clean_code == '':
        ms_clean_code = ms_code
    # initial code generated will be of the form <ms-name>-s<num>-v<sample_num>.py
    f = open(state["gen_dir"] + service_name + "-s" + sample_num + "-v" + version_num + ".py", "w+")
    print(f"writing generated code to file "
          f"{state['gen_dir'] + service_name + '-s' + sample_num + '-v' + version_num + '.py'}")
    f.write(ms_clean_code)
    f.close()
    state['ms_code'] = ms_clean_code
    return state


def runDockerAndTest(state):
    print("** Entering Agent RunDockerAndTest **")
    # (i) copy files to set up docker compose.  it will run the LLM-generated code for the microservice and the
    # ground truth for all the other services.
    # (ii) execute docker-compose to run the microservices.
    # (iii) run the tests on the microservices.  The tests are in the python script test_fn
    # (iv) store the run-time errors during testing. the test results are stored by the test script.
    # (v) remove docker containers
    state['errors'] = '' # reset as no errors yet for this code
    state['hasTestErrs'] = False
    state['hasRTerrs'] = False
    base_dir = state['base_dir']    # this is the directory that holds the GT code and dockerfile for each service
    gen_dir = state['gen_dir']      # this is the directory that stores the generated code
    service_name = state['service_name']
    # log_fn_prefix is the filename of the ms code, without the ".py" extension.   We will use this name to generate
    # other files -- e.g., log files describing errors found when running the ms code
    log_fn_prefix = state['service_name'] + "-s" + str(state['sample_num']) + "-v" + str(state['num_retries'])
    gen_service_prefix = gen_dir + log_fn_prefix  # gen_service_prefix is log_fn_prefix with full path
    print(f"base_dir = {base_dir} and gen_dir = {gen_dir} and service_name = {service_name}")

    # (i) copy program files

    shutil.copyfile(base_dir + "cardholders/GT-cardholders.py", base_dir + "cardholders/cardholders.py")
    # print(f"copied from {base_dir} + 'cardholders/GT-cardholders.py' to {base_dir} + 'cardholders/cardholders.py' ")
    shutil.copyfile(base_dir + "books/GT-books.py", base_dir + "books/books.py")
    shutil.copyfile(base_dir + "borrows/GT-borrows.py", base_dir + "borrows/borrows.py")
    shutil.copyfile(base_dir + "logs/GT-logs.py", base_dir + "logs/logs.py")
    # i-b.  copy the code for the generated service to the appropriate directory.  the generated code for service S has the name "S-vn.py", where n = version num
    base_filename = state["gen_dir"] + service_name + "-s" + str(state['sample_num']) + "-v" + str(state['num_retries'])
    shutil.copyfile(base_filename + ".py", base_dir + state["service_name"] + "/" + state["service_name"] + ".py")
    # print(f"copied from {base_filename} +'.py' to {base_dir} + {state['service_name']} + '/' + {state['service_name']} + '.py' ")

    # copy requirements files
    shutil.copyfile(base_dir + "cardholders/GT-requirements.txt", base_dir + "cardholders/requirements.txt")
    shutil.copyfile(base_dir + "books/GT-requirements.txt", base_dir + "books/requirements.txt")
    shutil.copyfile(base_dir + "borrows/GT-requirements.txt", base_dir + "borrows/requirements.txt")
    shutil.copyfile(base_dir + "logs/GT-requirements.txt", base_dir + "logs/requirements.txt")
    # generate requirements file for generated code and copy it to write directory
    try:
        h = open(gen_service_prefix + ".py", "r")
        prog = h.read()
        h.close()
        # the following line generates requirements.txt for prog and store it in directory base_dir + service_name
        generate_requirements_file(prog, base_dir + service_name)
    except Exception as e:
        print("************ Exception when generating requiorements ************ ")
        print("exception = ", e)

    # ii. remove old images and then run docker compose so that the services are running
    print("************ Removing any existing Docker microservice images ************")
    remove_old_image("generate-cardholders")
    remove_old_image("generate-books")
    remove_old_image("generate-borrows")
	remove_old_image("generate-logs")
	print("existing containers: ")
    # list_containers()
    print("attempting to delete containers mystate['docker_container_name']' and mystate['mongo_container_name'] ")
    # we remove all existing containers (not just the two stated above) to avoid naming issues.
    remove_all_containers()
    print("containers remaining after trying to delete containers")
    list_containers()
    

    print("************ Setting up containers using Docker Compose ************")
    try:
        # IMPORTANT NOTE.   This docker compose up cmd works correctly this cmd is run in the same directory that
        # contains the docker-compose.yml.  If you run it in a different directory (using -f parameter) as I tried in
        # the next line, it leaves a dangling container when issuing docker compose down, thereby also leaving the
        # mongo container running, and also leaving all images intact.   Therefore, we put the docker-compose.yml
        # in this directory.
        # output1 = subprocess.run(["docker-compose", "-f", "../docker-compose.yml", "up", "--build", "-d"],
        #                       capture_output=True, text=True)
        output1 = subprocess.run(["docker-compose", "up", "--build", "-d"], capture_output=True, text=True)
    except Exception as e:
        print("************ Exception when executing setting up Docker containers ************ ")
        print("exception = ", e)
        # print("Executing this process produced the following error messages:\n", output1.stderr)
    print("+++++ output1.returncode= ", str(output1.returncode))
    print("+++++ output1.stderr= ", output1.stderr)
    print("+++++ output1.stdout = ", output1.stdout)
    if "pip install" in output1.stderr:
        # then cannot install dependencies.   write to error file and exit
        fh = open(gen_service_prefix + "-runtime-errors.txt", "w")
        fh.write("Problem installing dependencies\n")
        fh.write("Dockerfile had issue with executing pip install")
        fh.write(output1.stderr)
        fh.close()
        print("output1.stdout= ", output1.stdout)
        return state
    # ii-b.  test that all the services are started
    loopnum = 0
    upAndRunnning = False
    while loopnum < 2 and not upAndRunnning:
        # Example usage (assuming 'docker' library is installed)
		container_name = mystate['docker_container_name']. # new  or old (next line)
        container_name = "/generate-" + state["service_name"] + "-1"

        if is_container_running(container_name):
            upAndRunnning = True
        else:
            print(f"Container '{container_name}' is not running")
        loopnum = loopnum + 1
        time.sleep(1)   # sleep for 1 second to give time for container to start

    # iii.Run tests on the microservices.
    try:
        print("************ Executing Tests ************")
        # run test program.   get error messages from stderr.   remove error messages about OpenSSL and warnings.
        # 1st parm (gen_dir) is directory to write output to.
        # 2nd parm (log_fn_prefix) is the name of the ms file without the extension.
        output2 = subprocess.run(["python", state['test_prog'], gen_dir, log_fn_prefix, "-d"],
                                 capture_output=True, text=True)
    except Exception as e:
        print("************ Exception when executing tests ************ ")
        print("exception = ", e)
    print("+++++ output2.returncode= ", str(output2.returncode))
    print("+++++ output2.stdout= ", output2.stdout)
    print("+++++ output2.stderr= ", output2.stderr)
    # iv.  retrieve the docker logs (output3logs) and store the results.  Note that:
    #      output1.stderr = messages printed by Docker on build.  such as "Container my-mongo-service  Creating".  Not
    #                       sure why these are in stderr and not in stdout.
    #      output1.stdout = Docker build messages on progress of build as it progresses
    #      output2.stdout = messages that the test script on execution prints to stdout, includes messages on tests
    #                       passed and tests that failed.
    #      output2.stderr = any error messages that python execution sends to stderr when executing the test script.
    #                       Usually this is just a warning the error messages (e.g., "NotOpenSSLWarning")
    #      output3.stdout = output printed by docker logs to stdout and stderr.  Often just be a couple of
    #                       standard messages such as "* Serving Flask app 'stocks.py' * Debug mode: on" and any output
    #                       the program itself prints out.
    #      output3.stderr = error messages from the microservices containers' logs.  This will contain the requests that
    #                       failed: "192.168.65.1 - - [25/Nov/2024 05:51:38] "POST /stocks HTTP/1.1" 400" but (not sure
    #                       why) can contain also successful messages:
    #                       "192.168.65.1 - - [25/Nov/2024 05:51:38] "GET /stocks HTTP/1.1" 200 -".
    #                       ***********   output3.stderr is important to us. It contains the runtime errors
    #      output3logs = output3.stderr but removing initial messages & returning only from "Traceback" errors & onward
    #      TracebackOutput3 = output3logs condensed as described below
    #      TracebackOutput2 = output2.stdout is truncated and condensed just like TracebackOutput3 for output3logs.
    #                         TracebackOutput2 is often empty
    #      tracebackErrs = "Runtime Errors when executing the Unit tests: \n" + TracebackOutput2 + TracebackOutput3.
    #                         This gets written to file with suffix "-runtime-errors.txt"
    #      state['errors'] =  tracebackErrs + the content of the file with suffix "-test-errors.txt" that was written by
    #                         the test script.
    #      output4.stderr = error messages from the docker compose down command.
    print("*********************************************************************************")
    print("*********************************************************************************")
    TracebackOutput2 = extract_errors(get_section(output2.stdout, start_pattern="Traceback", end_pattern=
                                                  "no more error messages", include_start_pattern=True),service_name)
    try:
        print("************ Retrieving docker logs ************ ")
        output3 = subprocess.run(["docker", "logs", container_name], capture_output=True, text=True)
        print("+++++ output3.stdout = ", output3.stdout)
        print("*********************************************************************************")
        print("+++++ output3.stderr = ", output3.stderr)
        print("*********************************************************************************")
        # remove the first part of the logs that are not error messages.  the end_pattern is random and this call will
        # therefore just truncate unneeded header info.
        output3logs = get_section(output3.stderr, start_pattern="Traceback", end_pattern="no more error messages",
                                  include_start_pattern=True)
        #   a. collect the output from the unit tests (on stdout).   extract the output starting from the keyword
        #      "Traceback". then "compress" these error messages (as it can be voluminous and contains information that
        #      is hard to use for debugging).  The compression finds lines containing "S.py", where S is the name of the
        #      service being generated (we assume the code being tested is in a file of that name). It will take the line
        #      containing "S.py" and the next 4 lines, as usually that suffices to describe the error.
        #   b. collect the docker log files from the docker container for S.    follow the same process as in (a) to
        #      collect and compress error messages.
        #      **** NEED TO CHECK that there is not overlap between (a) and (b)  ********
        #   c. append the error messages from (b) to those from (a) to form the runtime error messages RT. if RT is
        #      empty,there are no RT error messages.  If it is not empty, then set the error messages E to be RT.
        #   d. if there are not any RT errors, then assuming there are test errors TE (report from the unit tests on
        #      failed tests), set E to TE together with the direct output from the unit tests (the output it prints to
        #      stdout)
        # get_section will return '' if start and end patterns not found.  If "Traceback" not found, then output3logs
        # will equal '' (the empty string).
        print("output3.returncode= ", str(output3.returncode))
        print("output3logs =", output3logs)
        TracebackOutput3 = extract_errors(output3logs, service_name)
        tracebackErrs = "Runtime Errors when executing the Unit tests: \n" + TracebackOutput2 + TracebackOutput3
        print("++++++++++++++ tracebackErrs = ", tracebackErrs)
        if tracebackErrs == '':  # no runtime errors
            state['hasRTerrs'] = False
            print('no runtime errors')
        else:
            state['hasRTerrs'] = True
            print('has runtime errors')
        if state['hasRTerrs']:
            # if Python runtime errors, then set state['errors'] to these error messages
            state['errors'] = tracebackErrs
            fh = open(gen_service_prefix + "-runtime-errors.txt", "w")
            fh.write(tracebackErrs)
            fh.close()
        state['hasTestErrs'] = has_multiple_lines(gen_service_prefix + "-test-errors.txt")
        if state['hasTestErrs']:
            f = open(gen_service_prefix + "-test-errors.txt", "r")
            test_errors = f.read()
            f.close()
            state['errors'] = state['errors'] + "Unit test error messages:\n" + test_errors  # + unitTestsOutput
            # print("no RT error but has test errors = ")
            print(f"++++++++++++++++ state['errors'] = {state['errors']}")
    except Exception as e:
        print("************ Exception when executing tests ************ ")
        print("exception = ", e)


    # 5. kill and remove the containers
    try:
        print("************ Removing containers ************ ")
        print("images before docker compose down call")
        list_images()
        print("containers before docker compose down call")
        list_containers()
        output4 = subprocess.run(["docker-compose", "down", "--rmi", "all", "-v"], capture_output=True, text=True)
        print("output4.stderr = ", output4.stderr)
        print("output4.stdout = ", output4.stdout)
        print("output4.returncode= ", output4.returncode)
        print("images after docker compose down call")
        list_images()
        print("containers after docker compose down call")
        list_containers()
    except Exception as e:
        print("************ exception raised in removing containers ************")
        print(e)
        print("stderr = ", output4.stderr)
    return state


# Can add strategy that if # testing errors is decreasing, continue to try to correct code further
def chk4Err(state):
    print("** Entering Agent chk4Err **")
    if (state['hasRTerrs'] or state['hasTestErrs']):
        if state['num_retries'] < MAX_RETRIES:
            return "ReflectOnErr"
        else:
            print(f"Generated code {MAX_RETRIES} number of times and still have errors")
            return "FinalizeOut"
    else:
        print(f"Generated code without errors")
        meta_fn.close()
        return END


def reflectOnErr(state):
    print("** Entering Agent ReflectOnErr **")
    error_messages = state['errors']
    f = open(state['base_dir'] + "prompts/prompt_reflect.txt")
    prompt_reflect = f.read()
    f.close()
    # added the following lines to shorten error message becaues for gpt3.5 was getting the following error message:
    # openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16384
    # tokens. However, you requested 23319 tokens (7319 in the messages, 16000 in the completion). Please reduce the
    # length of the messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code':
    # 'context_length_exceeded'}}
    original_error_length = len(error_messages)
    shorten = False
    if GPT35:
        if original_error_length > 5000:
            error_messages = error_messages[0:5000]
            shorten = True
    else:  # GPT4, have less tokens
        if original_error_length > 4000:
            error_messages = error_messages[0:4000]
            shorten = True
    system_msg = {"role": "system",
                  "content": "The Python Interpreter tried to execute the following program: " + state['ms_code']
                             + " There errors were found during execution: " + error_messages}
    user_msg = {"role": "user",
                  "content": prompt_reflect}
    if REGEN_JUST_ERRS:
        messages = [system_msg, user_msg]
        meta_fn.write(f"Agent reflectOnErr: Only providing code and error messages to reflectOnErr LLM prompt.")
    else:
        messages = [state['initial_user_prompt']]
        messages.append(system_msg)
        messages.append(user_msg)
        meta_fn.write(f"Agent reflectOnErr: Providing service description in addition to code and error messages to reflectOnErr LLM prompt.")
    print("reflect messages = ", messages)
    response = client.chat.completions.create(messages=messages, model=MODEL, temperature=TEMP, max_tokens=MAX_TOKENS_2,
                                              n=1)
    print(" MAX_TOKENS_2 requests = ", MAX_TOKENS_2)
    print(f"Total tokens used = {response.usage.total_tokens}. Prompt tokens = {response.usage.prompt_tokens}, "
          f"Completion_tokens = {response.usage.completion_tokens}")
    meta_fn.write(f"Agent reflectOnErr\n"
                  f"   Length of original error_massage was {original_error_length}.   "
                  f"   Shorten = {str(shorten)}\n"
                  f"   MAX_TOKENS_1 requests =  {MAX_TOKENS_2}\n"
                  f"   Total tokens used = {response.usage.total_tokens}\n"
                  f"   Prompt tokens = {response.usage.prompt_tokens}, Completion_tokens = {response.usage.completion_tokens}\n"
                  f"   Temperature = {TEMP}\n\n"
                  )
    state['reflection'] = response.choices[0].message.content
    base_filename = state["gen_dir"] + state['service_name'] + "-s" + str(state['sample_num']) + "-v" + \
                          str(state['num_retries'])
    f = open(base_filename + "-reflect.txt", "w")
    f.write(state['reflection'])
    f.close()
    return state


def reGenPgm(state):
    print("** Entering Agent reGenPgm **")
    print("num_retries = ", state['num_retries'])
    system_msg2 = {"role": "system", "content": state['ms_code']}
    # error_message = state['errors']    # too long and gertting context too long error message
    reflection = state['reflection']
    reflection_content = f"When the Python Interpreter tried to execute this program,there were errors.   Here is the "\
                         f"summary and instructions on how to fix these errors: {reflection}.  Re-generate the code" \
                         f"and make sure to fix the errors. Do ot use CustomJSONEncoder.  Check that you have fixed"  \
                         f"each error."
    human_msg2 = {"role": "user",
                  "content": reflection_content}
    messages = [system_msg2,human_msg2]
    response = client.chat.completions.create(messages=messages, model=MODEL, temperature=TEMP, max_tokens=MAX_TOKENS_1,
                                              n=1)
    print(" MAX_TOKENS_1 requests = ", MAX_TOKENS_1)
    print(f"Total tokens used = {response.usage.total_tokens}. Prompt tokens = {response.usage.prompt_tokens}, "
          f"Completion_tokens = {response.usage.completion_tokens}")
    meta_fn.write(f"Agent reGenPgm\n"
                  f"   MAX_TOKENS_1 requests =  {MAX_TOKENS_1}\n"
                  f"   Total tokens used = {response.usage.total_tokens}\n"
                  f"   Prompt tokens = {response.usage.prompt_tokens}, Completion_tokens = \
                                       {response.usage.completion_tokens}\n"
                  f"   Temperature = {TEMP}\n\n"
                  )
    generated_code = response.choices[0].message.content
    # print("generated code = ", generated_code)
    state['num_retries'] += 1
    # last parameter is False so do not include start_pattern in returned code
    ms_clean_code = get_section(generated_code, "```python", "```", False)
    # if does not have "```python" prefix, then use raw generated code
    if ms_clean_code == '':
        ms_clean_code = generated_code
    base_filename = state["gen_dir"] + state['service_name'] + "-s" + str(state['sample_num']) + "-v" \
                    + str(state['num_retries'])
    f = open(base_filename + ".py", "w+")
    print(f"writing re-generated code to file {base_filename + '.py'}")
    f.write(ms_clean_code)
    f.close()
    state['ms_code'] = ms_clean_code
    # state['chat_history'] = state['chat_h  istory'].append({"role": "system", "content": state['ms_code']})
    return state



# FinalizeOut agent is entered if we are terminating the program but we still have errors.
def finalizeOut(state):
    print("** Entering Agent FinalizeOut **")
    error_messages = state['errors']
    f = open(state['base_dir'] + "prompts/prompt_reflect.txt")
    prompt_reflect = f.read()
    f.close()
    # added the following lines to shorten error message becaues for gpt3.5 was getting the following error message:
    # openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16384
    # tokens. However, you requested 23319 tokens (7319 in the messages, 16000 in the completion). Please reduce the
    # length of the messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code':
    # 'context_length_exceeded'}}
    original_error_length = len(error_messages)
    shorten = False
    if GPT35:
        if original_error_length > 5000:
            error_messages = error_messages[0:5000]
            shorten = True
    else:  # GPT4, have less tokens
        if original_error_length > 3500:
            error_messages = error_messages[0:3500]
            shorten = True
    system_msg = {"role": "system",
                  "content": "The Python Interpreter tried to execute the following program: " + state['ms_code']
                             + "There errors were found during execution: " + error_messages}
    human_msg = {"role": "user",
                 "content": prompt_reflect}
    messages = [state['initial_user_prompt'], state['initial_sys_prompt']]
    messages.append(system_msg)
    messages.append(human_msg)
    print("messages = ", messages)
    response = client.chat.completions.create(messages=messages, model=MODEL, temperature=TEMP, max_tokens=MAX_TOKENS_2,
                                              n=1)
    print(" MAX_TOKENS_2 requests = ", MAX_TOKENS_2)
    print(f"Total tokens used = {response.usage.total_tokens}. Prompt tokens = {response.usage.prompt_tokens}, "
          f"Completion_tokens = {response.usage.completion_tokens}")
    print(" MAX_TOKENS_2 requests = ", MAX_TOKENS_2)
    meta_fn.write(f"Agent finalizeOut\n"
                  f"   Length of original error_massage was {original_error_length}.   "
                  f"   Shorten = {str(shorten)}\n"
                  f"   MAX_TOKENS_1 requests =  {MAX_TOKENS_2}\n"
                  f"   Total tokens used = {response.usage.total_tokens}\n"
                  f"   Prompt tokens = {response.usage.prompt_tokens}, Completion_tokens = {response.usage.completion_tokens}\n"
                  f"   Temperature = {TEMP}\n\n"
                  )
    meta_fn.close()
    state['reflection'] = response.choices[0].message.content
    base_filename = state["gen_dir"] + state['service_name'] + "-s" + str(state['sample_num']) + "-v" + str(state['num_retries'])
    f = open(base_filename + "-reflect.txt", "w")
    f.write(state['reflection'])
    f.close()
    return state


# build langgraph graph
workflow = StateGraph(GraphState)
workflow.add_node("GraphInit", graphInit)
workflow.add_node("GenerateMicroservice",generateMicroservice)
workflow.add_node("RunDockerAndTest", runDockerAndTest)
workflow.add_node("ReflectOnErr",reflectOnErr)
workflow.add_node("ReGenPgm",reGenPgm)
workflow.add_node("FinalizeOut",finalizeOut)
workflow.set_entry_point("GraphInit")
# add edges
workflow.add_edge("GraphInit", "GenerateMicroservice")
workflow.add_edge("GenerateMicroservice", "RunDockerAndTest")
workflow.add_conditional_edges("RunDockerAndTest",chk4Err)
workflow.add_edge("ReflectOnErr", "ReGenPgm")
workflow.add_edge("ReGenPgm","RunDockerAndTest")
workflow.add_edge("FinalizeOut", END)


# compile and run tests
# perform test using 3 temperatures.  For each temperature perform NUM_TESTS
for TEMP in [0,.3,.5]:
# NUM_TESTS = 11 # perform 10 times, i=1 until i=11
    NUM_TESTS = 6
    for i in range(1,NUM_TESTS):
        runnable = workflow.compile()
        mystate = GraphState()
        d = datetime.now()
        curtime = d.strftime("%d-%m-%Y?-%M%S")
        base_dir = BASE_DIR
        gen_dir_prefix = GEN_DIR_PREFIX
        if INCLUDE_EX_CODE:
            ex = "-exmp"
        else:
            ex = "-noexmp"
        gen_dir =   base_dir + gen_dir_prefix + curtime + '-' + MODEL + '-temp' + str(TEMP) + '-1tok' + str(MAX_TOKENS_1) + '-2tok' + str(MAX_TOKENS_2)  + ex + '/'
        if not os.path.exists(base_dir + gen_dir_prefix):
            os.mkdir(base_dir + gen_dir_prefix)
        os.mkdir(gen_dir)
        meta_file = gen_dir  + '/meta_data'
        meta_fn = open(meta_file, 'w+')
        printdate = d.strftime("%d-%m-%Y-%H-%M")
        meta_fn.write(f"{printdate} Hours-Minutes\n"
                f"Model = {MODEL}\n"
                f"MAX_TOKENS_1 = {MAX_TOKENS_1}\n"
                f"MAX_TOKENS_2 = {MAX_TOKENS_2}\n"
                f"Temperature = {TEMP}\n"
        )
        if INCLUDE_EX_CODE:
            meta_fn.write("Using example service in prompt\n")
        else:
            meta_fn.write("Not using example service in prompt\n")
        for service_name in SERVICES:
            mystate['base_dir'] = base_dir
            mystate['gen_dir'] = gen_dir
            mystate["service_name"] = service_name
            mystate["sample_num"] = 1    # Since we currently generate just one sample per LLM call, not significant currently.
                                         # used as part of name of generated files.
            # getting the docker container name is important in order to check if ms container is running and then to remove it
            # when doen testing.   In this case, we invoke docker-compose from base_dir.  The Dockerfile for the ms is in the
            # "stocks" subdirectory of bse_dir.  The docker-compose.yml gives this service the name "portfolio".   We are only
            # running a single instance of this container.  Hence the name is '/stocks-portfolio-1'.  It turns out that you can
            # name containers in docker-compose.https://forums.docker.com/t/custom-container-name-for-docker-compose/48089/2
            # Hence in the docker-compose file i set the name to "my-stock-service".  Note that eventhough the docker-compose
            # file sets the name to "my-stock-service", docker gives it the name "/my-stock-service"
            # mystate['docker_container_name'] = '/stocks-portfolio-1'
            mystate['docker_container_name'] = DOCKER_CONTAINER_NAME
            mystate['mongo_container_name'] = MONGO_CONTAINER_NAME
            mystate['docker_image_name'] =  DOCKER_IMAGE_NAME
            print("base_dir = ", base_dir)
            print("gen_dir = ", gen_dir)
            print("service_name = ", service_name)
            print('docker_container_name = ', '/my-stock-service')
            runnable.invoke(mystate)
            print(f"$$$$$\nFinished test with service_name = {service_name}\n Test, number = {i}\n gen_dir = {gen_dir}\n$$$$$")
        meta_fn.close()
print("Done with tests")


